{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "July 28, 2016 - Women in Data Science Meetup - \"Data Science from Scratch\" Workshop #5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science from Scratch Tutorial\n",
    "## Naive Bayes\n",
    "### Julia Galstad 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to understand the Naive Bayes classifier found in chapter 13 of 'Data Science from Scratch' by Joel Grus. This project is creating a baby spam filter using the data from the SpamAssassin Public Corpus. https://spamassassin.apache.org/publiccorpus/\n",
    "\n",
    "To help familiarize with concepts and notation, there are some problems at the beginning that can be done 'by hand.' Please skip as much as you'd like.\n",
    "\n",
    "At the end of this notebook, there is a brief discussion of the distributions commonly used with Naive Bayes, and a small reference to one with a Guassian normal distribution to provide contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division                   \n",
    "from collections import defaultdict, Counter      \n",
    "import re                                   # for 'regular expression' operations\n",
    "import glob                                 # for work with pathname patterns\n",
    "import math, random\n",
    "import notebook_answers as ans              # not a standard python module\n",
    "                                                # contains hints and answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability and Bayes' Theorem\n",
    "### Cookie example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice applying Bayes' Theorem by solving the cookie problem formally found on <a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\" target=\"_blank\">this</a> Wikipedia page . (Thank you to Allen Downey for preserving it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two bowls of cookies:\n",
    "\n",
    "    Bowl #1 has 30 oatmeal cookies and 10 chocolate cookies. \n",
    "    \n",
    "    Bowl #2 has 20 oatmeal cookies and 20 chocolate cookies.\n",
    "    \n",
    "Someone gets an oatmeal cookie. What is the probability it is from Bowl #1?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before solving this question, let's calculate some other helpful probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>What is $P($Bowl #1$)$, the probability a cookie is from Bowl #1, before knowing it was oatmeal or chocolate? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can get a hint or the answer if you choose \n",
    "# and/or use the python notebook as your calculator.\n",
    "print(ans.hint(1))\n",
    "#print(ans.answer(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>What is $P($oatmeal$\\mid$Bowl #1$)$, the probability a cookie is oatmeal given that it came from Bowl #1? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ans.hint(2))\n",
    "#print(ans.answer(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>What is $P($oatmeal$)$, the probability a cookie is oatmeal? </font>\n",
    "\n",
    "This question can be solved in two ways. \n",
    "\n",
    "Let's practice using the Law of Total Probability: $$P(E) = P(H)P(E\\mid H) + P(\\neg H)P(E\\mid \\neg H).$$\n",
    "\n",
    "In our cookie example you would calculate:\n",
    "\n",
    "$$P({\\rm oatmeal}) = P({\\rm Bowl}\\,{\\rm \\#1})P({\\rm oatmeal}\\mid {\\rm Bowl}\\,{\\rm \\#1 }) + P({\\rm Bowl}\\,{\\rm\\#2})P({\\rm oatmeal }\\mid{\\rm Bowl}\\,{\\rm \\#2 }).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ans.hint(3))\n",
    "#print(ans.answer(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we're nearly ready to put it all together to answer the main question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <font color='blue'>What is $P($ Bowl #1$\\mid$oatmeal $)$, the probability that the cookie was from Bowl #1, given that it's oatmeal? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use Bayes' Theorem to solve this. \n",
    "\n",
    "We can state Bayes' Theorem as\n",
    "\n",
    "$$ P(H \\mid E) = \\frac{P(H)P(E \\mid H)}{P(E)}$$\n",
    "\n",
    "where $H$ is the hypothesis, and $E$ is the evidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the hypothesis, $H$, is:\n",
    "\n",
    "    'the cookie is from Bowl #1' \n",
    "and the evidence, $E$, is:\n",
    "\n",
    "    'the cookie is oatmeal.' \n",
    "    \n",
    "We could write out Bayes' Theorem for our example like this:\n",
    "$$P({\\rm Bowl}\\,{\\rm \\#1} \\mid{\\rm oatmeal}) = \\frac{P({\\rm Bowl}\\,{\\rm \\#1})P({\\rm oatmeal}\\mid {\\rm Bowl}\\,{\\rm \\#1 })}{P({\\rm oatmeal})}.$$\n",
    "\n",
    "Recall, we want to solve for $P($ Bowl #1$\\mid$oatmeal $)$, the probability that the cookie was from Bowl #1, given that it's oatmeal.\n",
    "\n",
    "We've already calculated the values we need to plug into Bayes' Theorem, so now let's do just that: substitute the values into the right hand side of the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ans.hint(4))\n",
    "#print(ans.answer(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian interpretation: accounting for new evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'In the Bayesian (or epistemological) interpretation, probability measures a degree of belief. Bayes' theorem then links the degree of belief in a proposition before and after accounting for evidence.' (<a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\" target=\"_blank\">Wikipedia</a>)\n",
    "\n",
    "In the cookie example, our original hypothesis was the belief that the cookie was from Bowl #1. We then found new evidence, that it was oatmeal. We then updated our belief to include this new information using Bayes' Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>What kind of evidence do you think makes sense to use to determine if an email is spam or not? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may discuss your ideas with your group and/or post them to the slack channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Intuition towards a Naive Bayes spam classifier\n",
    "### Using words as evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our spam filter will use the occurence of a word in a message as evidence.\n",
    "\n",
    "Specifically, if our training set emails contain $n$ distinct words, $w_1, \\dots, w_n$, we will have $n$ features/events $X_1, \\dots, X_n$, where $X_i$ stands for 'a message contains the word $w_i$.' We are assuming (which is false, as described in the talk) that the features are independent. \n",
    "\n",
    "We call the set of words $w_1, \\dots, w_n$ a $vocabulary$. \n",
    "\n",
    "For each word $w_i$, we calculate conditional probabilities of that word occuring in a spam message and in a non-spam message, $P(X_i\\mid S)$ and $P(X_i\\mid \\neg S)$, respectively, where $S$ is spam.\n",
    "\n",
    "To calculate this values, we need to determine which words occur in which emails.\n",
    "\n",
    "### Tokenizing messages\n",
    "\n",
    "Given some email data set, we want to $tokenize$ each message. For us, that means to turn each message into a collection of unique lowercase words that the message was comprised of. (If you're familiar with NLP, you know you would want to throw out function words like $of$, $the$. Our simple ``tokenize`` function doesn't do that.) \n",
    "\n",
    "The function ``tokenize`` from Joel Grus is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words, can also use \\w as the filter\n",
    "    return set(all_words)                           # remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of ``re.findall``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument, ``[a-z0-9']+``, that we pass to ``findall`` is a <a href=\"https://en.wikipedia.org/wiki/Regular_expression\" target=\"_blank\">regular expression</a>, which is a specific syntax that represents a pattern that we want our text to match. \n",
    "\n",
    "Using ``[a-z0-9']+`` indicates we are looking for words that contain the symbols: ``a-z0-9'``. \n",
    "\n",
    "The ``+`` symbol indicates we want words and not just letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.findall(\"[a-z0-9]\", \"abc's:of testING;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.findall(\"[a-z0-9']+\", \"abc's:of testING;42Sup 42sup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of ``tokenize``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenize('\"We should start back\", Gared urged as the woods began to grow'\n",
    "         'dark around them. \"The wildlings are dead.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Just for fun -- Who wrote these sentences? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ans.answer(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to estimate conditional probabilities, we count the occurences of the words.\n",
    "\n",
    "The function ``count_words`` by Joel Grus tokenizes a message and then 'return a dictionary whose keys are words, and whose values are two-element lists ``[spam_count, non_spam_count]`` corresponding to how many times we saw that word in both spam and nonspam messages.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(training_set):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A toy example -- from ``count_words`` to computing conditional probabiities\n",
    "\n",
    "Let's pretend that we just ran ``count_words`` on a small training set (with 400 spams and 425 non-spams), and returned this dictionary, which we saved as ``winter_dict``.\n",
    "\n",
    "The dictionary keys are the words of our vocabulary.\n",
    "The dictionary value associated with its key (a word) is a list that contains the total number of spam emails in which that word occured and the total number of non-spam emails in which that word occured. \n",
    "\n",
    "``{``'``word``': [``spam_count``, ``non_spam_count``]``}``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "winter_dict = {'hodor': [100, 400], 'winter': [50, 20], 'witch': [300, 10], 'red': [10, 100], 'dragon': [0, 10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's think about how to calculate the conditional probabilities we'll need for Naive Bayes using a dictionary like this. Take 5 minutes to answer three questions about ``winter_dict``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>1. How many words are in our vocabulary? What are they? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ans.answer(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>2. Using the dictionary ``winter_dict``, estimate $P(X_1 \\mid S)$. That is, estimate $P($ 'hodor' $\\mid$ spam $)$, the probability that a message has the word ``'hodor'``, given that we know it is spam. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ans.hint(11))\n",
    "#print(ans.answer(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (We're using the symbol $\\neg$ to indicate negation. This way we can write 'not spam' as $\\neg S$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>3. Using the dictionary ``winter_dict``, estimate $P(X_1 \\mid \\neg S)$. That is, estimate $P($ 'hodor' $\\mid$ not spam $)$, the probability that a message has the word ``'hodor'``, given that we know the message is not spam. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ans.hint(12))\n",
    "#print(ans.answer(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we've calculated these conditional probabilities that we'll need for Naive Bayes, we can make a function that calculates them automatically.\n",
    "\n",
    "\n",
    "We define a function ``word_probabilities_prototype`` that assigns conditional probabilites to words in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_probabilities_prototype(counts, total_spams, total_non_spams):\n",
    "    \"\"\"Turn the word_counts into a list of triplets\n",
    "    word, p(word | spam) and p(word | not spam)\"\"\"\n",
    "    return [(word,\n",
    "             spam / total_spams ,\n",
    "             non_spam / total_non_spams)\n",
    "             for word, (spam, non_spam) in counts.iteritems()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the function ``word_probabilities_prototype`` to our toy dictionary ``winter_dict``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "winter_word_probabilities = word_probabilities_prototype(winter_dict, 400, 425)\n",
    "print(winter_word_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A word of caution: Dictionaries don't preserve order.\n",
    "\n",
    "When ``word_probabilities_prototype`` iterates through key-value pairs in ``winter_dict``, the word associated with the second feature might not appear second in the list ``winter_word_probabilities``. \n",
    "\n",
    "In fact, for me, I decided the word associated with feature $X_2$ is `'winter'`, but ``winter_words_probabilties[2]=('winter', 0.125, 0.047058823529411764)``.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's classify some messages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start off with the message ``'Hodor Hodor Bran'``, which in our test set isn't spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our features are the presence (or absence) of our vocabulary words in the message, so the fact that ``'Hodor'`` appears twice, or the fact that the word ``'Bran'`` appears at all, are irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What features represent this message?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about the vocabulary of our model, the probability that the message ``'Hodor Hodor Bran'`` is spam (and this is an abuse of notation here) is:\n",
    "$$P({\\rm Spam }\\mid {\\rm hodor = True, winter = False, witch = False, red = False, dragon = False})$$\n",
    "\n",
    "In appropriate notation, this is the probability we are estimating:\n",
    "$$P(S \\mid X_1, \\neg X_2, \\neg X_3, \\neg X_4, \\neg X_5)$$\n",
    "\n",
    "As shorthand, you could write: $$P(S|E)$$ where $E$ stands for the evidence described by our features $X_1, \\neg X_2, \\neg X_3, \\neg X_4, \\neg X_5$.\n",
    "\n",
    "We then apply Bayes' Theorem to get:\n",
    "$$P(S \\mid E) = \\frac{P(S)P(E\\mid S)}{P(E)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of Spam Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assume that a message is equally likely to be spam or not spam, so we set $P(S) = P (\\neg S) = 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Most implementations of Naive Bayes don't make this assumption.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Law of Total Probability\n",
    "\n",
    "We use the law of total probability, $P(E) = P(S)P(E\\mid S) + P(\\neg S)P(E\\mid \\neg S)$, to transform the denominator. \n",
    "$$P(S \\mid E) = \\frac{P(S)P(E\\mid S)}{P(S)P(E\\mid S) + P(\\neg S)P(E\\mid \\neg S)}$$\n",
    "\n",
    "We did this because our assumption that $P(S)=P(\\neg S)=0.5$ was helpful; these terms cancel out and can be ignored. \n",
    "\n",
    "$$P(S \\mid E) = \\frac{{\\rm \\bf{0.5}}\\cdot P(E\\mid S)}{{\\rm \\bf{0.5}}\\cdot P(E\\mid S) + {\\rm \\bf{0.5}}\\cdot P(E\\mid \\neg S)}$$\n",
    "\n",
    "$$P(S \\mid E) = \\frac{{\\rm \\bf{0.5}}\\cdot P(E\\mid S)}{{\\rm \\bf{0.5}}\\cdot \\left(P(E\\mid S) +  P(E\\mid \\neg S)\\right)}$$\n",
    "\n",
    "$$P(S \\mid E) = \\frac{P(E\\mid S)}{P(E\\mid S) + P(E\\mid \\neg S)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule simplification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the probability $P(E\\mid S)$ on the right-hand side. (The simplifications with $P(E\\mid \\neg S)$ are similar.)\n",
    "\n",
    "Recall, the evidence from our example message ``'Hodor Hodor Bran'`` was $X_1, \\neg X_2, \\neg X_3, \\neg X_4, \\neg X_5$, so $$P(E\\mid S)=P(E\\mid X_1, \\neg X_2, \\neg X_3, \\neg X_4, \\neg X_5).$$\n",
    "\n",
    "\n",
    "Apply Chain Rule to get:\n",
    "$$P(E \\mid S) = P(X_1 \\mid S)P(\\neg X_2 \\mid X_1,S)P(\\neg X_3 \\mid X_1, \\neg X_2,S)P(\\neg X_4 \\mid X_1, \\neg X_2, \\neg X_3,S), P(\\neg X_5 \\mid X_1, \\neg X_2, \\neg X_3, \\neg X_4,S)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes assumption\n",
    "\n",
    "With our (false) Naive Bayes assumption that the presence of these words are conditionally independent, we ignore the effects of the other words:\n",
    "\n",
    "$$P(E \\mid S) = P(X_1\\mid S)P(\\neg X_2\\mid S)P(\\neg X_3\\mid S)P(\\neg X_4\\mid S)P(\\neg X_5 \\mid S)$$\n",
    "\n",
    "Our formula is now: $$P(S \\mid E) = \\frac{P(E\\mid S)}{P(E\\mid S) + P(E\\mid \\neg S)}$$\n",
    "\n",
    "$$P(S \\mid E) = \\frac{P(X_1\\mid S)P(\\neg X_2\\mid S)P(\\neg X_3\\mid S)P(\\neg X_4\\mid S)P(\\neg X_5 \\mid S)}{{\\rm same} \\,{\\rm as} \\, {\\rm numerator} + {\\rm similar} \\, {\\rm to} \\, {\\rm  numerator}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Our list ``winter_word_probabilities`` contains $P(X_1\\mid S) = 0.25$, the probability of ``'hodor'`` given that the message is spam. </font>\n",
    "\n",
    "### <font color='blue'>Can we use ``winter_word_probabiiltes`` to calculate the other probabilities in the formula, such as $P(\\neg X_2 \\mid S)$? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: We'll need to use: $P(\\neg X \\mid S ) = 1 - P(X \\mid S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we remember that $P(X) + P(\\neg X) = 1$. In a more useful form,  $P(\\neg X) = 1 - P(X)$. This applies to conditional probability, too: $$P(\\neg X \\mid S) = 1 - P(X \\mid S)$$\n",
    "\n",
    "Our list contains $P(X_2 \\mid S)=0.125$, the probability of ``'winter'`` given that the mesage is spam. \n",
    "\n",
    "Therefore: $$P(\\neg X_2 \\mid S) = 1 - 0.125 = 0.875$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> To check your understanding, use the numbers in ``winter_word_probabilities`` to verify that the numerator of this formula evaluates to $0.25 \\cdot (1 - 0.125) \\cdot (1 - 0.75) \\cdot (1 - 0.025) \\cdot (1 - 0)$: </font>\n",
    "\n",
    "$$P(S \\mid E) = \\frac{P(X_1\\mid S)(1- P(X_2\\mid S))(1 -P(X_3\\mid S))(1 -P( X_4\\mid S))(1 - P(X_5 \\mid S))}{{\\rm same} \\,{\\rm as} \\, {\\rm numerator} + {\\rm similar} \\, {\\rm to} \\, {\\rm  numerator}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hodor', 0.25, 0.9411764705882353), ('witch', 0.75, 0.023529411764705882), ('winter', 0.125, 0.047058823529411764), ('red', 0.025, 0.23529411764705882), ('dragon', 0.0, 0.023529411764705882)]\n"
     ]
    }
   ],
   "source": [
    "# To help you with the above verification, again we can look at the \n",
    "# list titled winter_word_probabilities containing the triples:\n",
    "# [word, P(word | spam), P(word | not spam)]\n",
    "print(winter_word_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This formula that we just verified is going to be the crux of the main function in our Naive Bayes classifier.\n",
    "\n",
    "To implement this formula, we define the function ``spam_probability_prototype``, which estimates the probability our message is spam given the evidence provided by our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spam_probability_prototype(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    message_prob_if_spam = message_prob_if_not_spam = 1.0\n",
    "\n",
    "    # iterate through each word in our vocabulary\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "\n",
    "        # if *word* appears in the message,\n",
    "        # multiply the probability of seeing it\n",
    "        if word in message_words:\n",
    "            message_prob_if_spam *= prob_if_spam\n",
    "            message_prob_if_not_spam *= prob_if_not_spam\n",
    "\n",
    "        # if *word* doesn't appear in the message\n",
    "        # multiply the probability of _not_ seeing it\n",
    "        # which is (1 - probability of seeing it)\n",
    "        else:\n",
    "            message_prob_if_spam *= (1.0 - prob_if_spam)\n",
    "            message_prob_if_not_spam *= (1.0 - prob_if_not_spam)\n",
    "\n",
    "    return message_prob_if_spam / (message_prob_if_spam + message_prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Do we classify the message 'Hodor Hodor Bran' as spam or not spam? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(spam_probability_prototype(winter_word_probabilities, 'Hodor Hodor Bran'))\n",
    "#print(ans.answer(13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argmax\n",
    "\n",
    "### <font color='blue'> In the general Naive Bayes algorithm, the classification step uses an argmax function as a part of the decision rule. Why didn't we do that here? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(ans.answer(13.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the usual implementation of Naive Bayes, we leave out the denominator of Bayes' Theorem.\n",
    "### <font color='blue'> If we omit the denomiator, can we still leave out argmax? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ans.hint(13.7))\n",
    "#print(ans.answer(13.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>How do we classify the message 'witch red winter'? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(spam_probability_prototype(winter_word_probabilities, 'witch red winter'))\n",
    "# print(ans.answer(14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> How do we classify the message 'witch red winter dragon'? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(spam_probability_prototype(winter_word_probabilities, 'witch red winter dragon'))\n",
    "#print(ans.answer(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> What happened? Why would adding the word 'dragon' change the probabilities so much? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(ans.answer(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Why is this a problem? Do you see any other problems with our model? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving our Naive Bayes ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this 0-annihilation problem, we will change our word_probabilities_prototype function to include smoothing with a $pseudocount$. It's time to hallucinate!\n",
    "\n",
    "\"When computing the spam probabilities for the $i$th word, we assume we also saw $k$ additional spams containing the word and $k$ additional spams not containing the word.\" (Joel Grus)\n",
    "\n",
    "\"If 'dragon' occurs in 0/98 spam documents, and if k is 1, we estimate $P($ dragon $\\mid$ spam $)$  as 1/100 = 0.01, which allows our classifier to still assign some nonzero spam probability to messages that contain the word 'dragon.'\" (a quote of Joel Grus that was dragon-ified)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function  ``word_probabilities`` calculates conditional word probabilities using a $pseudocount$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets \n",
    "    w, p(w | spam) and p(w | ~spam)\"\"\"\n",
    "    return [(w,\n",
    "             (spam + k) / (total_spams + 2 * k),\n",
    "             (non_spam + k) / (total_non_spams + 2 * k))\n",
    "             for w, (spam, non_spam) in counts.iteritems()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Using ``word_probabilities``, how should we classify the message 'witch red winter dragon'? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ans.hint(17))\n",
    "# print(ans.hint(17.1))\n",
    "# print(ans.answer(17))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculations with small numbers\n",
    "\n",
    "There's another issue we need to fix in our model. (It's not a problem in our tiny five-word vocabulary example.) Suppose we have a very large vocabulary, which includes some words with extremely high word counts and other words with extremely low word counts. We could be potentially multiplying together lots of very small numbers when we calculate probabilities.\n",
    "\n",
    "### <font color='blue'> What might go wrong? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'In practice, you usually want to avoid multiplying lots of probabilities together, to avoid a problem called $underflow$, in which computers don’t deal well with floating-point numbers that are too close to zero.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'We usually compute a product of probabilities $p_1p_2 \\cdots p_n$ as the equivalent (but floating-point-friendlier): $$\\exp(\\log(p_1)+\\log(p_2)+\\cdots + \\log(p_n)).'$$\n",
    "(Joel Grus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula comes from combining the facts that the exponential and logarithm functions are inverses: $$\\exp(\\log(x))=x$$ and that in log space, you can compute multiplications as additions: $$\\log(ab)=\\log(a)+\\log(b).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The updated ``spam_probability`` function that takes $underflow$ into account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "\n",
    "        # for each word in the message, \n",
    "        # add the log probability of seeing it \n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "\n",
    "        # for each word that's not in the message\n",
    "        # add the log probability of _not_ seeing it\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "            \n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the pieces together into a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "\n",
    "    def train(self, training_set):\n",
    "    \n",
    "        # count spam and non-spam messages\n",
    "        num_spams = len([is_spam \n",
    "                         for message, is_spam in training_set \n",
    "                         if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "\n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts, \n",
    "                                             num_spams, \n",
    "                                             num_non_spams,\n",
    "                                             self.k)\n",
    "                                             \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model on SpamAssassin data\n",
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use just the subject lines of the emails from the files prefixed with 20021010.\n",
    "\n",
    "You'll need to change the code below based on where you've saved the data on your machine.\n",
    "\n",
    "The command 'pwd' will tell you the current path of this notebook.\n",
    "\n",
    "If you've saved the three folders (spam, easy_ham, hard_ham) in a 'data' folder, and 'data' is in the same place as this notebook, then putting the path from 'pwd' in front of /data/\\*/\\* will probably work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify the path with wherever you've put the files\n",
    "# there should be 3 folders: spam, easy_ham, hard_ham\n",
    "# Below assumes they are stored in one folder named 'data'\n",
    "# \n",
    "# the command 'pwd' will tell you the current path of this notebook and\n",
    "# might be what you put in front of /data/*/*\n",
    "# \n",
    "path = r\"C:\\data\\*\\*\"\n",
    "\n",
    "data = []\n",
    "\n",
    "# glob.glob returns every filename that matches the wildcarded path\n",
    "for fn in glob.glob(path):\n",
    "    is_spam = \"ham\" not in fn\n",
    "\n",
    "    with open(fn,'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                # remove the leading \"Subject: \" and keep what's left\n",
    "                subject = re.sub(r\"^Subject: \", \"\", line).strip()\n",
    "                data.append((subject, is_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Re: New Sequences Window', False), ('[zzzzteana] RE: Alexander', False), ('[zzzzteana] Moscow bomber', False), (\"[IRR] Klez: The Virus That  Won't Die\", False), ('Re: Insert signature', False)] [('Re: Adult Classifieds', True), ('Low Price Smokes', True), ('Low Price Tobacco', True), ('REQUEST FOR MUTUALLY BENEFITTING ENDEAVOUR.', True), (\"** You're -Approved-!\", True)]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at part of the data\n",
    "print data[0:5], data[3000:3005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data, prob):\n",
    "    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results\n",
    "\n",
    "random.seed(0)                                  # just so you get the same answers as Joel\n",
    "train_data, test_data = split_data(data, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a classifier with our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "Test to see how well the model is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 704, (True, True): 101, (True, False): 38, (False, True): 33})\n"
     ]
    }
   ],
   "source": [
    "# triplets (subject, actual is_spam, predicted spam probability)\n",
    "classified = [(subject, is_spam, classifier.classify(subject))\n",
    "              for subject, is_spam in test_data]\n",
    "\n",
    "# assume that spam_probability > 0.5 corresponds to spam prediction\n",
    "# and count the combinations of (actual is_spam, predicted is_spam)\n",
    "counts = Counter((is_spam, spam_probability > 0.5)\n",
    "                 for _, is_spam, spam_probability in classified)\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'This gives 101 true positives (spam classified as “spam”), 33 false positives (ham classified as “spam”), 704 true negatives (ham classified as “ham”), and 38 false negatives (spam classified as “ham”). This means our precision is 101 / (101 + 33) = 75%, and our recall is 101 / (101 + 38) = 73%, which are not bad numbers for such a simple model.' (Grus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Precision$ and $recall$ are common measurements for classification problems. See <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" target=\"_blank\"> here </a> for more info. For us, on the image below, the relevant elements are the spam emails.\n",
    "\n",
    "<img src=images/Precisionrecall.png align=\"center\" width=\"40%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'It’s also interesting to look at the most misclassified:' (Grus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort by spam_probability from smallest to largest\n",
    "classified.sort(key=lambda row: row[2])\n",
    "\n",
    "# the highest predicted spam probabilities among the non-spams\n",
    "spammiest_hams = filter(lambda row: not row[1], classified)[-5:]\n",
    "\n",
    "# the lowest predicted spam probabilities among the actual spams\n",
    "hammiest_spams = filter(lambda row: row[1], classified)[:5]\n",
    "\n",
    "print(spammiest_hams)\n",
    "print(hammiest_spams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'The two spammiest hams both have the words “needed” (77 times more likely to appear in spam), “insurance” (30 times more likely to appear in spam), and “important” (10 times more likely to appear in spam).\n",
    "\n",
    "The hammiest spam is too short (“Re: girls”) to make much of a judgment, and the second-hammiest is a credit card solicitation most of whose words weren’t in the training set.' (Grus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now check out the 'spammiest' words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('year', 0.028767123287671233, 0.00022893772893772894), ('sale', 0.031506849315068496, 0.00022893772893772894), ('rates', 0.031506849315068496, 0.00022893772893772894), ('systemworks', 0.036986301369863014, 0.00022893772893772894), ('money', 0.03972602739726028, 0.00022893772893772894)]\n",
      "[('spambayes', 0.0013698630136986301, 0.04601648351648352), ('users', 0.0013698630136986301, 0.036401098901098904), ('razor', 0.0013698630136986301, 0.030906593406593408), ('zzzzteana', 0.0013698630136986301, 0.029075091575091576), ('sadev', 0.0013698630136986301, 0.026785714285714284)]\n"
     ]
    }
   ],
   "source": [
    "def p_spam_given_word(word_prob):\n",
    "    \"\"\"uses bayes's theorem to compute p(spam | message contains word)\"\"\"\n",
    "\n",
    "    # word_prob is one of the triplets produced by word_probabilities\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
    "\n",
    "spammiest_words = words[-5:]\n",
    "hammiest_words = words[:5]\n",
    "print(spammiest_words)\n",
    "print(hammiest_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> What ways can you think of to improve this spam filter? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some suggestions from Grus:\n",
    "\n",
    "$\\cdot$ Modify the classifier to accept an optional min_count threshhold and ignore tokens that don’t appear at least that many times.\n",
    "\n",
    "$\\cdot$ We could add extra features like “message contains a number” by creating phony tokens like contains:number and modifying the tokenizer to emit them when appropriate.\n",
    "\n",
    "$\\cdot$ The tokenizer has no notion of similar words (e.g., “cheap” and “cheapest”). Modify the classifier to take an optional stemmer function that converts words to equivalence classes of words. For example, a really simple stemmer function might be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_final_s(word):\n",
    "    return re.sub(\"s$\", \"\", word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> What issues might come up if you include the whole message and not just the subject? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief word about probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributions describe what kind of outcomes we expect a random variable to take. For more information, check out <a href=\"http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf\" target=\"_blank\"> this</a> open source probability book. \n",
    "\n",
    "We make assumptions about these distributions when we design our model. \n",
    "\n",
    "Scikit-learn implements <a href=\"http://scikit-learn.org/stable/modules/naive_bayes.html\" target=\"_blank\">three different versions</a> of Naive Bayes, based on either Bernoulli, a Multinomial or Gaussian distribution.\n",
    "\n",
    "We assumed our data followed a <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\" target=\"_blank\">Bernoulli distribution</a>.\n",
    "\n",
    "In contrast, <a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gender_classification\" target=\"_blank\">this gender classification example</a> follows a Gaussian distribution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
